{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ET091_08_Gesture\n",
    "\n",
    "## Model Training\n",
    "#### 1. Use 20BN Dataset.\n",
    "#### 2. Filter 6 gesture.\n",
    "   * Swiping Left\n",
    "   * Swiping Right\n",
    "   * Swiping Down \n",
    "   * Swiping Up\n",
    "   * Turning Hand Clockwise\n",
    "   * Turning Hand Counterclockwise\n",
    "\n",
    "#### script run\n",
    "   * Following path [./Model_Training/datasetfilter.ipynb](./Model_Training/datasetfilter.ipynb)\n",
    "\n",
    "#### 3. Preprocessing\n",
    "   * Preprocessing Dataset info for training and testing\n",
    "\n",
    "   * Training Data info Structure\n",
    "     EX: /GestureDataset/B/TurningHandClockwise/37958/ 36 4\n",
    "     1. Data Folder Directory:/GestureDataset/B/TurningHandClockwise/37958/\n",
    "     2. Amount of frames in the folder:36\n",
    "     3. Class of this folder:4\n",
    "\n",
    "   * Testing Data info Structure\n",
    "     EX: /GestureDataset/B/TurningHandClockwise/47908/ 9 4 35\n",
    "     1. Data Folder Directory:/GestureDataset/B/TurningHandClockwise/47908/\n",
    "     2. Middle frame index:9\n",
    "     3. Class of this folder:4\n",
    "     4. Amount of frames in the folder:35\n",
    "\n",
    "#### script run\n",
    "   * Change Path first\n",
    "     * data_path (Read image data path)\n",
    "     * out_filename (Store data info path)\n",
    "   * Following path [./Model_Training/datapreprocessing.ipynb](./Model_Training/datapreprocessing.ipynb)\n",
    "\n",
    "#### 4. Training\n",
    "\n",
    "   * \"pretrained\" Folder include pretrain weight (Sports1M and UCF 101)\n",
    "\n",
    "   * 5 type model training\n",
    "       * with different data sampling, data augmentation, training batch, training epochs and network structure.\n",
    "         1. K_3DCNN_v1\n",
    "         2. K_3DCNN_v2\n",
    "         3. K_3DCNN_v3\n",
    "         4. K_3DCNN_v4\n",
    "         5. K_3DCNN-svm-xgb\n",
    "         6. K_3DCNN_v42\n",
    "\n",
    "#### script run\n",
    "   * Following path [Model Detail](./Model_Training/readme.md)\n",
    "\n",
    "\n",
    "## Model Testing\n",
    "1. Different model performance testing\n",
    "  * A. K_3DCNN_Testing\n",
    "    * \"testdata\" is testing data folder\n",
    "    * \"Keras_Model\" include a load model package (a class base package include load model and predict method) \n",
    "    * Following path [./Model_Testing/K_3DCNN_Testing.ipynb](./Model_Testing/K_3DCNN_Testing.ipynb)\n",
    "    \n",
    "  * B. K_3DCNN_Validate\n",
    "    * Compare different model performance (Include Loss, Accuracy, Precision, Recall, f1-score)\n",
    "    * 20BN Data validation => '/list/c3d_val001.txt'\n",
    "    * Real Data validation => '/list/c3d_test005.txt'\n",
    "    * Following path [./Model_Testing/K_3DCNN_Validate.ipynb](./Model_Testing/K_3DCNN_Validate.ipynb)\n",
    "\n",
    "## Simple Demo\n",
    "\n",
    "1. INFO\n",
    "  * \"model\" folder include a model file.\n",
    "  * \"libs\" folder include a web browser to youtube player package\n",
    "  * Follow \"requirements.txt\" to build  demo code environmental requirement\n",
    "    * Install Python 3.7.7 at local PC or NB environment instead of using anaconda\n",
    "    * Build virtual python environment, then follow \"requirements.txt\" to install package dependency\n",
    "    * Use CUDA 10.1 and cudnn v7.6.5 \n",
    "\n",
    "2. DEMO\n",
    "    1. Simple_Demo\n",
    "      * simple demo with webcam capture, model predict and keyboard trigger\n",
    "      * Following path [Simple_Demo](./Simple_Demo/Simple_Demo.ipynb)\n",
    "\n",
    "    2. Demo\n",
    "      * automatic open web browser and go to youtube player(Press 'S' to start open web browser)\n",
    "      * connect between webcam capture, model predict(Press 'P' to start predict)\n",
    "      * use model predict result to control web browser to youtube player\n",
    "      * Following path [Demo](./Simple_Demo/Demo.ipynb)\n",
    "      \n",
    "## Hand_Detect_Software\n",
    "### 以下為軟體操作說明\n",
    "* 1. Python環境設置\n",
    "* 2. App 使用說明\n",
    "\n",
    "### 1. Python環境設置\n",
    "* 1.0 安裝Python 3.7.x版本\n",
    "* 1.1 安裝外部模組\n",
    "  * 1.1.1 請修改Python目錄下install_all_modules.bat中\"Python安裝路徑中Script路徑\"\n",
    "  * 1.1.2 請執行Python目錄下install_all_modules.bat\n",
    "* 1.2 將自訂義的模組\"Demo\\libs\"加入到Python安裝路徑中\n",
    "  * 1.2.1 請修改cam_demo_lib.pth中自訂義的模組路徑\n",
    "  * 1.2.2 請複製cam_demo_lib.pth到Python安裝路徑中下的\"Lib\\site-packages\"中\n",
    "* 1.3 開發專案\n",
    "  * 1.3.1 請修改open_jupyter_lab.bat中\"工作目錄\"和\"Python安裝路徑中jupyter-lab路徑\"\n",
    "  * 1.3.2 請開啟\"Demo\\Demo.ipynb\"進行修改, 修改完成匯出Demo.py並覆蓋\n",
    "\n",
    "### 2. App 使用說明\n",
    "* 2.1 功能\n",
    "  * 2.1.1 擷取Cam影像進行手勢辨識同時控制youtube 360影片\n",
    "  * 2.1.2 手勢種類與影片對應\n",
    "    * 2.1.2.1 左: 影片轉左\n",
    "    * 2.1.2.2 右: 影片轉右\n",
    "    * 2.1.2.3 上: 影片轉上\n",
    "    * 2.1.2.4 下: 影片轉下\n",
    "    * 2.1.2.5 順時針: 放大影片\n",
    "    * 2.1.2.6 逆時針: 縮小影片\n",
    "* 2.2 執行App\n",
    "  * 2.2.1 請修改main.bat中\"工作目錄下Demo路徑\"和Python安裝路徑\n",
    "  * 2.2.2 請執行main.bat\n",
    "* 2.3 關閉App\n",
    "  * 2.3.1 點擊鍵盤中\"q\"鍵\n",
    "* 2.4 更換youtube 360影片\n",
    "  * 2.4.1 修改Demo\\Demo.py中的youtube_360_video_url變數\n",
    "* 2.5 如要更換應用\n",
    "  * 2.5.1 請繼承Demo\\libs\\desktop_lib.py進行實作\n",
    "  * 2.5.2 增加Demo\\libs\\threed_cnn_lib.py中類別Three_D_CNN_Thread的mode變數\n",
    "  * 2.5.3 修改Demo\\Demo.py中threed_cnn_mode變數\n",
    "      \n",
    "\n",
    "      \n",
    "## 20BN Gesture\n",
    "##### Swiping Left\n",
    "![Swiping Left.](./GestureGif/TWBN/SwipingLeft.gif \"Swiping Left\") \n",
    "##### Swiping Right\n",
    "![Swiping Right.](./GestureGif/TWBN/SwipingRight.gif \"Swiping Right\")\n",
    "##### Swiping Down\n",
    "![Swiping Down.](./GestureGif/TWBN/SwipingDown.gif \"Swiping Down \")\n",
    "##### Swiping Up\n",
    "![Swiping Up.](./GestureGif/TWBN/SwipingUp.gif \"Swiping Up\")\n",
    "##### Turning Hand Clockwise\n",
    "![TurningHandClockwise.](./GestureGif/TWBN/TurningHandClockwise.gif \"Turning Hand Clockwise\")\n",
    "##### Turning Hand Counterclockwise\n",
    "![Turning Hand Counterclockwiset.](./GestureGif/TWBN/TurningHandCounterclockwise.gif \"Turning Hand Counterclockwise\")\n",
    "\n",
    "## Testing Gesture\n",
    "##### Swiping Left\n",
    "![Swiping Left.](./GestureGif/RealData/SwipingLeft.gif \"Swiping Left\") \n",
    "##### Swiping Right\n",
    "![Swiping Right.](./GestureGif/RealData/SwipingRight.gif \"Swiping Right\")\n",
    "##### Swiping Down\n",
    "![Swiping Down.](./GestureGif/RealData/SwipingDown.gif \"Swiping Down \")\n",
    "##### Swiping Up\n",
    "![Swiping Up.](./GestureGif/RealData/SwipingUp.gif \"Swiping Up\")\n",
    "##### Turning Hand Clockwise\n",
    "![TurningHandClockwise.](./GestureGif/RealData/TurningHandClockwise.gif \"Turning Hand Clockwise\")\n",
    "##### Turning Hand Counterclockwise\n",
    "![Turning Hand Counterclockwiset.](./GestureGif/RealData/TurningHandCounterclockwise.gif \"Turning Hand Counterclockwise\")\n",
    "\n",
    "\n",
    "## Model Difference\n",
    "* 前、中、後 => 各資料夾 Frame 總數的 1/4、1/2、3/4 索引值為基準，各向前、後取得連續 8 Frames，構成總數為 16 Frames的訓練輸入資料\n",
    "* A、N => 各資料夾最前與最後 Frame 重複取6次，再向後與向前取得連續 10 Frames，構成總數為 16 Frames的訓練輸入資料\n",
    "* 中2 => 各資料夾 Frame 總數的 1/2 索引值為基準，向前、後推進 16 個索引值，向前 16 個索引值為起始值，向後 16 個索引值為終止值，每間隔一個索引值取一個 Frame，構成總數為 16 Frames的訓練輸入資料\n",
    "\n",
    "\n",
    "| Model  | batch  | Epochs | Train and Valid Data          | Option                                                                             | .h5 file                                     |\n",
    "| ------ |:------:|:------:|:-----------------------------:|:----------------------------------------------------------------------------------:|:--------------------------------------------:|\n",
    "| 1      | 32     | 50     | (9600+2400)x3(前、中、後)      | X                                                                                  | basic_model-best-model.h5                    |\n",
    "| 2      | 32     | 20     | (9600+2400)x3(前、中、後)      | X                                                                                  | basic_model-best-model-rlf.h5                |\n",
    "| 3      | 32     | 30     | (9600+2400)x3(前、中、後)      | X                                                                                  | basic_model-best-model-2020-10-08_17-28-03.h5|\n",
    "| 4      | 32     | 30     | (9600+2400)x5(A、前、中、後、N)| Add drop between Conv3a to Conv3b and Conv4a to Conv4b, FC6 and FC7 add L1, L2     | basic_model-best-model-2020-10-09_20-11-11.h5|\n",
    "| 5      | 32     | 30     | (9600+2400)x5(A、前、中、後、N)| Add drop between Conv3a to Conv3b and Conv4a to Conv4b, FC6 and FC7 add L1, L2     | basic_model-best-model-2020-10-11_16-41-12.h5| \n",
    "| 6      | 32     | 50     | (9600+2400)x5(A、前、中、後、N)| Add drop between Conv3a to Conv3b and Conv4a to Conv4b, FC6 and FC7 add L1, L2     | basic_model-best-model-2020-10-12_20-28-06.h5| \n",
    "| 7      | 64     | 30     | (9600+2400)x5(A、前、中、後、N)| Add drop between Conv3a to Conv3b and Conv4a to Conv4b, FC6 and FC7 add L1, L2     | basic_model-best-model-2020-10-16_10-10-14.h5| \n",
    "| 8      | 32     | 30     | (9600+2400)x5(A、前、中、後、N)| Add drop between Conv4b to Conv5a and Conv5a to Conv5b, FC6 and FC7 add L1, L2     | basic_model-best-model-2020-10-18_00-57-57.h5|   \n",
    "\n",
    "\n",
    "| Model|Batch|Epochs| Train and Valid Data          | Option                                                                            | .h5 file                                     |\n",
    "| ---- |:---:|:----:|:-----------------------------:|:---------------------------------------------------------------------------------:|:--------------------------------------------:|\n",
    "| 1(C7)| 32  | 30   |(11200+2800)x3 (前、中、後)     | 1. Right-left, Up-Down flip. 2.dropout at Conv3a-3b, Conv4a4b. FC6-FC7 add L1, L2 | basic_model-best-model-2020-10-24_21-50-57.h5|\n",
    "| 2(C7)| 32  | 30   |(11200+2800)x3 (前、中、後)     | 1. Right-left, Up-Down flip. 2.dropout at Conv3a-3b, Conv4a4b. FC6-FC7 add L1, L2 | basic_model-best-model-2020-10-25_09-31-51.h5|\n",
    "| 3(C6)| 32  | 30   |(9600+2400)x5(A、前、中2、後、N)| 1. Right-left, Up-Down flip. 2.dropout at Conv3a-3b, Conv4a4b. FC6-FC7 add L1, L2 | basic_model-best-model-2020-10-25_20-11-36.h5|\n",
    "| 4(C7)| 32  | 30   |(11200+2800)x3 (前、中、後) Gray| 1. Right-left, Up-Down flip. 2.dropout at Conv3a-3b, Conv4a-4b. FC6-FC7 add L1, L2| basic_model-best-model-2020-10-26_14-19-30.h5|\n",
    "| 5(C7)| 32  | 30   |(11200+2800)x3(前、中2、後)     | 1. Right-left, Up-Down flip. 2.dropout at Conv3a-3b, Conv4a-4b. FC6-FC7 add L1, L2| basic_model-best-model-2020-10-27_07-21-28.h5|\n",
    "| 6(C7)| 32  | 30   |(11200+2800)x3(前、中2、後) Gray| 1. Right-left, Up-Down flip. 2.dropout at Conv3a-3b, Conv4a-4b. FC6-FC7 add L1, L2| basic_model-best-model-2020-10-27_16-19-10.h5|\n",
    "\n",
    "\n",
    "\n",
    "## Team Member Assignment of Responsibility\n",
    "\n",
    "* ET091007 林銘彥\n",
    "    1. Model reference survey\n",
    "    2. Implement training model and validatate model \n",
    "\n",
    "* ET0910012 陳俊安\n",
    "    1. Paper 彙整\n",
    "    2. 簡報與海報製作\n",
    "    3. Training Dataset 彙整 / Real Testing Data Record\n",
    "\n",
    "* ET0910021 劉皓瑋\n",
    "    1. Paper 彙整\n",
    "    2. 簡報與海報製作\n",
    "    3. Training Dataset 篩選 / Real Testing Data Record\n",
    "\n",
    "* ET091023 蔡明佑\n",
    "    1. Video stream as model input technic survey\n",
    "    2. Implement model prediction to application "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Model Testing\n",
    "\n",
    "1. load model from path \"./model/XXX.h5\"\n",
    "\n",
    "\n",
    "2. K_3DCNN_Testing\n",
    "* [K_3DCNN_Testing](./K_3DCNN_Testing.ipynb)\n",
    "\n",
    "a. \"testdata\" is testing data folder\n",
    "b. Keras_Model include a load model package \n",
    "c. K_3DCNN_Testing.ipynb use two way to test model\n",
    "\n",
    "3. K_3DCNN_Validate\n",
    "* [K_3DCNN_Validate](./K_3DCNN_Validate.ipynb)\n",
    "\n",
    "a. Compare different model performance\n",
    "b. 20BN Data validation => './list/c3d_val001.txt'\n",
    "c. Real Data validation => './list/c3d_test005.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. INFO\n",
    "\n",
    "  * \"model\" folder include a model file.\n",
    "  * \"libs\" folder include a web browser to youtube player package\n",
    "  * Follow \"requirements.txt\" to build  demo code environmental requirement\n",
    "    * Install Python 3.7.7 at local PC or NB environment instead of using anaconda\n",
    "    * Build virtual python environment, then follow \"requirements.txt\" to install package dependency\n",
    "    * Use CUDA 10.1 and Cudnn 7.6.5. \n",
    "\n",
    "### 2. DEMO\n",
    "\n",
    "  * Simple_Demo\n",
    "    * [Simple_Demo](./Simple_Demo.ipynb)\n",
    "    * simple demo with webcam capture, model predict and keyboard trigger\n",
    "\n",
    "  * Demo\n",
    "    * [Demo](./Demo.ipynb)\n",
    "    * automatic open web browser to youtube player(Press 'S' to start open web browser)\n",
    "    * connect between webcam capture, model predict(Press 'P' to start predict)\n",
    "    * use model predict result to control web browser to youtube player\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
